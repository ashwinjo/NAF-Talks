import React from 'react'
import MarkdownRenderer from './MarkdownRenderer'

const Day2Advanced = () => {
  const content = `## Day 2 â€” Advanced Track

### **Session 1: Scaling Network Operators with Modular Ansible**

**Speaker:** Joseph Nicholson (NTT Data)

Joseph Nicholson presented on NTT Data's approach to creating a **Modular Ansible Framework** for multi-environment automation, drawing from his experience in the NOC and data center.

**NTT Data Network Context:**

* **Scale:** Network spans 5 continents, with over 90 PoPs in 40+ countries, and hundreds of backbone routers.
* **Tooling:** Utilizes a blend of **Ansible** (e.g., for router upgrades) and **Python**, with operations running from the CLI or **Ansible AWX**.

**Repository Management (Key Recommendation):**

* Joseph recommends implementing a strict **naming policy for repositories**, as the names are used frequently in operational tasks and discussions.

**Ansible Playbook Structure:**

* Playbooks run directly from the Git repository.
* Focus is on **inventory and tasks**.
* Repositories contain sub-folders named after playbooks, further divided by **Network Operating System (NOS)** folders.
* **Shared tasks** (e.g., circuit testing, collection, router NOS upgrades, traffic updates) are reused across all playbooks.

**CI/CD Pipeline & Workflow Control:**

* The pipeline is a two-stage process: **FIX ME** check and **AWX Updates**.
* **FIX ME Tags:** Used to mark problems or incomplete tasks. A stage in the **Merge Request** pipeline looks for \`FIX ME\` tags and **blocks the merge** until they are removed.

**Ansible Inventory Management:**

* **Source:** Inventory is dynamically generated from an in-house, legacy Change Management system: **GNI Unified Management System (GUMS)** (an actively maintained system since the 1990s).
* **Dynamic Script:** The Ansible inventory is generated by a **dynamic Python script** that runs in milliseconds. It builds the inventory based on groups, dumps it to memory, and makes it available globally to Ansible.
* **Data Variables:** Variables include hostname, platform, vendor, OS, state (up/down), and geographical data (PoP, country, region).

**AWX Inventory Automation:**

* An altered version of the dynamic inventory script runs hourly as a **CI/CD pipeline** job to update the **AWX inventory YAML files**.
* This was automated because updates to the central Ansible inventory previously required manual updates to AWX.

**Modular Tasks (Reusability):**

* **Task files** are collections of tasks designed to perform a complete process.
* These files are **reusable** across different playbooks.
* They run their tasks when called and do not require redirection back to the original calling playbook.

***

### **Session 2: Building a Source of Truth for Brownfield Networks (Revelo)**

**Speaker:** Munachimso (Muna) Nwaiwu (Network Automation Engineer / Ph.D. Student)

Muna Nwaiwu presented **Revelo**, a project aimed at helping engineers with **brownfield networks** establish a foundational **Source of Truth (SoT)**, enabling subsequent automation efforts.

**The Brownfield Challenge:**

* Brownfield networks typically suffer from **ancient, undocumented devices** and years of **configuration drift**.
* Waiting for a greenfield opportunity is not feasible.

**Starting Automation:**

* The most fundamental element to start with is the **Source of Truth**.
* A centralized, model-based data source is essential for building automation effectively.

**Revelo's Three Major Steps (Bare Minimum Approach):**

Revelo focuses on discovering network information to bootstrap the SoT.

* **Step 1:** Bootstrap inventory.
* **Step 2:** Collect Source of Truth model.

**The Discovery Path (Path 2):**

1.  **Input:** Provide IP and credentials.
2.  **Discovery:** The Revelo script runs **nmap** to automatically discover the device platform.
3.  **Expansion:** A subnet can be provided to discover hidden devices (e.g., servers).

**Underlying Technologies:**

The code behind Revelo leverages established tools, including **Napalm, Netmiko, and Genie from pyATS**.

**How SoT Unlocks Automation (Model-Driven Approach):**

Once the structured data is collected, it unlocks key automation capabilities:

* **Auditing:** Run reports to compare the **live network** against the **network model** to expose differences.
* **Foundation:** Stop relying on unstructured data like spreadsheets.
* **Structure:** Build a versioned, structured SoT.
* **Validation:** Unlock automated auditing.

***

### **Session 3: Bridging Infrastructure and Data with Terraform & Infrahub**

**Speaker:** Marco Martinez (Swisscom)

Marco Martinez presented a session on **"Bridging Infrastructure and Data,"** focusing on the use of **Infrastructure as Code (IaC)** for networking using **Terraform** (or OpenTofu).

**Key Tool Integration:**

* Marco detailed his automation pipeline, which integrates **Terraform (or OpenTofu)** with tools like **GraphQL** and **Infrahub**.
* The presentation included practical details on integrating these elements to establish a functional automation pipeline.

**Bonus Project Mention:** Marco also co-hosts a podcast episode detailing the **Network Unit Testing System (NUTS)** project.

***

### **Session 4: Model Context Protocol (MCP) vs. APIs**

**Speaker:** William Collins (Itential)

William Collins explored the purpose and controversy surrounding the **Model Context Protocol (MCP)**, which has been available for about a year.

**What is MCP?**

* MCP is an **open protocol** designed to enable the integration between **Large Language Model (LLM) applications** and external data sources and tools.

**The Need and Controversy:**

* **Controversy:** When MCP first emerged, there was skepticism regarding its necessity ("Do we really need this?").
* **Arguments Against:** Some view MCP servers as mere wrappers around existing APIs or a gimmick, arguing that it offers nothing beyond standard function-calling.
* **Arguments For (Network Engineer Perspective):** MCP provides a **standard** for connecting diverse AI applications (Claude, ChatGPT, Gemini, Llama) to various tools and data sources (Salesforce, Slack, GitHub). This standard reduces the need for costly **custom integrations** for every AI-to-data source combination.

**API vs. MCP Comparison (Choosing the Right Tool):**

William provided a comparison to highlight the architectural differences, emphasizing that the choice depends on the specific use case:

| Feature | RESTful APIs | Model Context Protocol (MCP) |
| :--- | :--- | :--- |
| **Architecture** | Client-server | Model-driven (for AI context) |
| **Transport** | HTTP | JSON-RPC (among others) |
| **Connection State** | **Stateless** request-and-response model | **Stateful** |
| **Purpose** | General-purpose data exchange | Standardized **AI-to-Tool/Data** integration |`

  return (
    <section id="day2-advanced" className="relative py-20 px-4 sm:px-6 lg:px-8 bg-terminal-darker bg-opacity-30">
      <div className="max-w-6xl mx-auto">
        <div className="mb-8">
          <div className="flex items-center space-x-3 mb-4">
            <span className="text-4xl">ðŸ”§</span>
            <h2 className="text-3xl font-bold text-terminal-green">Day 2 - Advanced Track</h2>
          </div>
          <p className="text-terminal-gray font-mono text-sm">$ ls -la day2_advanced/</p>
        </div>
        
        <div className="border border-terminal-green-dim bg-terminal-bg bg-opacity-50 rounded-lg p-8 backdrop-blur-sm">
          <MarkdownRenderer content={content} />
        </div>
      </div>
    </section>
  )
}

export default Day2Advanced

